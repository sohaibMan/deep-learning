{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "from pandas import read_csv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "# dataset definition preparation \n",
    "class CSVDataset(Dataset):\n",
    "    # load the dataset\n",
    "    def __init__(self, path):\n",
    "        # load the csv file as a dataframe\n",
    "        self.df = read_csv(path)\n",
    "\n",
    "        self.df.head()\n",
    "        # Drop the date column\n",
    "        # self.df = self.df.drop('date', axis=1)\n",
    "        # Drop the symbol column\n",
    "        self.df = self.df.drop('symbol', axis=1)\n",
    "        # store the inputs and outputs\n",
    "        self.X = self.df.values[:, :-1]\n",
    "        self.y = self.df.values[:, -1]\n",
    "        # ensure target has the right shape\n",
    "        self.y = self.y.reshape((len(self.y), 1))\n",
    "\n",
    "    # number of rows in the dataset\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # get a row at an index\n",
    "    def __getitem__(self, idx):\n",
    "        return [self.X[idx], self.y[idx]]\n",
    "\n",
    "    # get indexes for train and test rows\n",
    "    def get_splits(self, n_test=0.33):\n",
    "        # determine sizes\n",
    "        test_size = round(n_test * len(self.X))\n",
    "        train_size = len(self.X) - test_size\n",
    "        # calculate the split\n",
    "        return random_split(self, [train_size, test_size])\n",
    "\n",
    "    def print_info(self):\n",
    "        print(self.df.info())\n",
    "        print(self.df.describe())\n",
    "        print(self.df.head())\n",
    "        print(self.df.isnull().sum())\n",
    "\n",
    "    def visualize(self, column):\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.countplot(y=column, data=self.df)\n",
    "        plt.title('Spans from 2010 to the end 2016')\n",
    "        plt.show()\n",
    "\n",
    "    def check(self):\n",
    "        # Step 3: Check for missing values\n",
    "        print(self.df.isnull().sum())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T21:39:47.264772Z",
     "start_time": "2024-03-10T21:39:47.248222Z"
    }
   },
   "id": "21143567cd556d5b",
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "(12288, 45)"
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a=np.random.randn(12288,150)\n",
    "b=np.random.randn(150,45)\n",
    "c=np.dot(a,b)\n",
    "c.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-17T15:40:17.908587Z",
     "start_time": "2024-03-17T15:40:17.775648Z"
    }
   },
   "id": "acdbbc6bed22502a",
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 1: Load the dataset\n",
    "data_set = CSVDataset('data/prices.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T21:39:49.533214Z",
     "start_time": "2024-03-10T21:39:48.272437Z"
    }
   },
   "id": "2f2810fefabbe46d",
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 851264 entries, 0 to 851263\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count   Dtype  \n",
      "---  ------  --------------   -----  \n",
      " 0   date    851264 non-null  object \n",
      " 1   open    851264 non-null  float64\n",
      " 2   close   851264 non-null  float64\n",
      " 3   low     851264 non-null  float64\n",
      " 4   high    851264 non-null  float64\n",
      " 5   volume  851264 non-null  float64\n",
      "dtypes: float64(5), object(1)\n",
      "memory usage: 39.0+ MB\n",
      "None\n",
      "                open          close            low           high  \\\n",
      "count  851264.000000  851264.000000  851264.000000  851264.000000   \n",
      "mean       70.836986      70.857109      70.118414      71.543476   \n",
      "std        83.695876      83.689686      82.877294      84.465504   \n",
      "min         0.850000       0.860000       0.830000       0.880000   \n",
      "25%        33.840000      33.849998      33.480000      34.189999   \n",
      "50%        52.770000      52.799999      52.230000      53.310001   \n",
      "75%        79.879997      79.889999      79.110001      80.610001   \n",
      "max      1584.439941    1578.130005    1549.939941    1600.930054   \n",
      "\n",
      "             volume  \n",
      "count  8.512640e+05  \n",
      "mean   5.415113e+06  \n",
      "std    1.249468e+07  \n",
      "min    0.000000e+00  \n",
      "25%    1.221500e+06  \n",
      "50%    2.476250e+06  \n",
      "75%    5.222500e+06  \n",
      "max    8.596434e+08  \n",
      "                  date        open       close         low        high  \\\n",
      "0  2016-01-05 00:00:00  123.430000  125.839996  122.309998  126.250000   \n",
      "1  2016-01-06 00:00:00  125.239998  119.980003  119.940002  125.540001   \n",
      "2  2016-01-07 00:00:00  116.379997  114.949997  114.930000  119.739998   \n",
      "3  2016-01-08 00:00:00  115.480003  116.620003  113.500000  117.440002   \n",
      "4  2016-01-11 00:00:00  117.010002  114.970001  114.089996  117.330002   \n",
      "\n",
      "      volume  \n",
      "0  2163600.0  \n",
      "1  2386400.0  \n",
      "2  2489500.0  \n",
      "3  2006300.0  \n",
      "4  1408600.0  \n",
      "date      0\n",
      "open      0\n",
      "close     0\n",
      "low       0\n",
      "high      0\n",
      "volume    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Summarize the dataset\n",
    "data_set.print_info()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-10T21:38:11.253576Z",
     "start_time": "2024-03-10T21:38:10.965643Z"
    }
   },
   "id": "c640e4c76b97e9af",
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 3: Visualize the dataset\n",
    "data_set.visualize('open')"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-03-10T21:39:52.788026Z"
    }
   },
   "id": "663427805d37b492",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 4: Check for missing values\n",
    "data_set.check()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "9e6d85c84433b8ab",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as func\n",
    "\n",
    "\n",
    "# Step 5: Define the DNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, n_neurons=10):\n",
    "        super(Net, self).__init__()\n",
    "        # Define the layers of the network here\n",
    "        self.fc1 = nn.Linear(5, n_neurons)\n",
    "        self.fc2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.fc3 = nn.Linear(n_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Define the forward pass\n",
    "        x = func.relu(self.fc1(x))\n",
    "        x = func.relu(self.fc2(x))\n",
    "        # no activation function for the la\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "13f2aa4136e7e042",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import SGD\n",
    "from torch.nn import MSELoss\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Step 6: Create an instance of the network\n",
    "model = Net()\n",
    "\n",
    "\n",
    "# Step 7: Train the model\n",
    "def train_model(train_data, training_model):\n",
    "    size = len(train_data.dataset)\n",
    "    # define the optimization\n",
    "    criterion = MSELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "    # enumerate epochs\n",
    "    # enumerate epochs\n",
    "    for epoch in tqdm(range(100), desc='Training Epochs'):\n",
    "        print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "        # enumerate mini batches\n",
    "        for batch, (inputs, targets) in enumerate(train_data):\n",
    "            # clear the gradients\n",
    "            optimizer.zero_grad()\n",
    "            # compute the model output\n",
    "            yhat = training_model(inputs)\n",
    "            # calculate loss\n",
    "            loss = criterion(yhat, targets)\n",
    "            # credit assignment\n",
    "            loss.backward()\n",
    "            # update model weights\n",
    "            optimizer.step()\n",
    "\n",
    "            #if batch % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(inputs)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "b8be6151d6f344fa",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 8: Prepare the data\n",
    "def prepare_data(dataset):\n",
    "    # calculate split\n",
    "    train, test = dataset.get_splits()\n",
    "    # prepare data loaders\n",
    "    return DataLoader(train, batch_size=1024, shuffle=True), DataLoader(test, batch_size=1024, shuffle=False)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "c1ba0d06e53310dc",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Step 9: Train the model\n",
    "train_dl, test_dl = prepare_data(data_set)\n",
    "print(len(train_dl.dataset), len(test_dl.dataset))\n",
    "\n",
    "train_model(train_dl, model)\n"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1206287323dd2c1c",
   "execution_count": null
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from skorch import NeuralNetRegressor\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Step 10: Define a function to create a PyTorch model with variable parameters\n",
    "def create_model(n_neurons=10):\n",
    "    return Net(n_neurons=n_neurons)\n",
    "\n",
    "\n",
    "# Step 11: Define a function to perform grid search\n",
    "def perform_grid_search(X, y):\n",
    "    # Define the hyperparameters\n",
    "    hyperparameters = {\n",
    "        'module__n_neurons': [10, 20, 30],\n",
    "        'lr': [0.01, 0.001, 0.0001],\n",
    "        'max_epochs': [10, 50, 100],\n",
    "        'optimizer': [optim.SGD, optim.Adam],\n",
    "    }\n",
    "\n",
    "    # Create a Skorch neural network object\n",
    "    net = NeuralNetRegressor(module=create_model, criterion=nn.MSELoss, iterator_train__shuffle=True)\n",
    "\n",
    "    # Create a GridSearchCV object\n",
    "    grid_search = GridSearchCV(net, hyperparameters, cv=3, scoring='neg_mean_squared_error')\n",
    "\n",
    "    # Fit the GridSearchCV object\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Print the best parameters\n",
    "    print(\"Best parameters found: \", grid_search.best_params_)\n",
    "\n",
    "    return grid_search.best_estimator_\n",
    "\n",
    "\n",
    "# Step 12: Call the grid search function with the desired parameters\n",
    "best_model = perform_grid_search(data_set.X, data_set.y)"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "bd08999c05aacba3"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Step 13: Initialize lists to store loss and accuracy values\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "\n",
    "# Step 14: Train the model\n",
    "def test_model(test_data, testing_model):\n",
    "    size = len(test_data.dataset)\n",
    "    # define the optimization\n",
    "    criterion = MSELoss()\n",
    "    current_test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in test_data:\n",
    "            # compute the model output\n",
    "            outputs = testing_model(inputs)\n",
    "            # calculate loss\n",
    "            current_test_loss += criterion(outputs, targets).item()\n",
    "            # calculate accuracy\n",
    "            pred = outputs.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(targets.view_as(pred)).sum().item()\n",
    "\n",
    "    current_test_loss /= size\n",
    "    current_test_accuracy = correct / size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100 * current_test_accuracy):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "    return test_loss, test_accuracy\n",
    "\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Train the model\n",
    "    train_loss, train_accuracy = train_model(train_dl, model)\n",
    "    # Test the model\n",
    "    test_loss, test_accuracy = test_model(test_dl, model)\n",
    "    # Store the loss and accuracy values\n",
    "    train_losses.append(train_loss)\n",
    "    test_losses.append(test_loss)\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "\n",
    "# Step 15: After training, use matplotlib to plot the loss and accuracy values\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train')\n",
    "plt.plot(test_losses, label='Test')\n",
    "plt.title('Loss / Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train')\n",
    "plt.plot(test_accuracies, label='Test')\n",
    "plt.title('Accuracy / Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "eeca4a7ce36634e7"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "class NetWithRegularization(nn.Module):\n",
    "    def __init__(self, n_neurons=10, dropout_rate=0.5, *args, **kwargs):\n",
    "        super(NetWithRegularization, self).__init__()\n",
    "        self.fc1 = nn.Linear(5, n_neurons)\n",
    "        self.dropout1 = nn.Dropout(dropout_rate)\n",
    "        self.fc2 = nn.Linear(n_neurons, n_neurons)\n",
    "        self.dropout2 = nn.Dropout(dropout_rate)\n",
    "        self.fc3 = nn.Linear(n_neurons, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = func.relu(self.fc1(x))\n",
    "        x = self.dropout1(x)\n",
    "        x = func.relu(self.fc2(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "def train_model(train_data, validation_data, training_model, l1_lambda=0.005):\n",
    "    criterion = MSELoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.01, momentum=0.9, weight_decay=0.01)  # L2 regularization\n",
    "    best_val_loss = float('inf')\n",
    "    for epoch in tqdm(range(100), desc='Training Epochs'):\n",
    "        # Training Phase \n",
    "        training_model.train()\n",
    "        for inputs, targets in train_data:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = training_model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            l1_norm = sum(p.abs().sum() for p in model.parameters())  # L1 regularization\n",
    "            loss = loss + l1_lambda * l1_norm\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, targets in validation_data:\n",
    "                outputs = training_model(inputs)\n",
    "                val_loss += criterion(outputs, targets).item()\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "        else:\n",
    "            print(\"Early stopping\")\n",
    "            break"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "5a1946af31e2fefb"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "# Test the original model\n",
    "test_loss_original, test_accuracy_original = test_model(test_dl, model)\n",
    "print(f\"Original Model - Test Loss: {test_loss_original}, Test Accuracy: {test_accuracy_original}\")\n",
    "\n",
    "# Create an instance of the network with regularization\n",
    "model_with_regularization = NetWithRegularization()\n",
    "\n",
    "# Train the model with regularization\n",
    "train_model(train_dl, test_dl, model_with_regularization)\n",
    "\n",
    "# Test the model with regularization\n",
    "test_loss_regularized, test_accuracy_regularized = test_model(test_dl, model_with_regularization)\n",
    "print(f\"Model with Regularization - Test Loss: {test_loss_regularized}, Test Accuracy: {test_accuracy_regularized}\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   },
   "id": "1758c75bc54c50ba"
  },
  {
   "cell_type": "markdown",
   "source": [
    "we can summarize the first part of the lab in those steps:  \n",
    "\n",
    "1. A CSV dataset is loaded and preprocessed. The date and symbol columns are dropped, and the remaining data is split into inputs (X) and targets (y).\n",
    "\n",
    "2. A neural network architecture is defined with three fully connected layers.\n",
    "\n",
    "3. The model is trained using Stochastic Gradient Descent (SGD) and Mean Squared Error (MSE) loss.\n",
    "\n",
    "4. A grid search is performed to find the best hyperparameters for the model.\n",
    "\n",
    "5. The model's performance is evaluated on the test data.\n",
    "\n",
    "6. A new model is defined with dropout layers for regularization. The training function is also modified to include L1 and L2 regularization and early stopping.\n",
    "\n",
    "7. The regularized model is trained and evaluated on the test data, and its performance is compared with the original model.\n",
    "\n",
    "8. The loss and accuracy of the models are plotted for each epoch.\n"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8819bbfbddd29185"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "f0a7961a4d6650f4"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
